# Q5: T5 Fine-tuning - Table Format for LaTeX

## Table 3: Details of the best-performing T5 model configurations (fine-tuned)

| Design choice | Description |
|--------------|-------------|
| **Data processing** | Natural language queries are prepended with the prefix "translate English to SQL: " before tokenization to provide task specification. Both encoder inputs and decoder targets are truncated to a maximum length of 512 tokens. End-of-sequence (EOS) tokens are explicitly appended to decoder targets if not present. During batch collation, sequences are dynamically padded: encoder/decoder inputs use pad_token_id (0), while decoder targets are padded with -100 to ignore padded positions in loss computation. Decoder inputs are created by shifting targets by one position and prepending pad_token_id as the initial token (teacher-forcing). No additional preprocessing (lowercasing, formatting) is applied. |
| **Tokenization** | T5TokenizerFast from HuggingFace Transformers (google-t5/t5-small) is used for both encoder and decoder tokenization. This SentencePiece-based tokenizer handles both natural language and SQL syntax effectively. Encoder inputs are tokenized after prefix addition with truncation to max_length=512. Decoder targets (SQL queries) are tokenized directly without prefix, also with max_length=512. Special tokens include pad_token_id (0) for padding and EOS token (1) for sequence termination. The default T5 tokenizer is used without modifications as it naturally handles SQL syntax through its learned subword vocabulary. |
| **Architecture** | T5ForConditionalGeneration (small variant, 60M parameters) from google-t5/t5-small is fine-tuned. The model consists of 6 encoder layers, 6 decoder layers, 8 attention heads, and hidden dimension 512. Full model fine-tuning is performed: all parameters in encoder (6 transformer layers with self-attention, feed-forward networks, layer normalization), decoder (6 transformer layers with self-attention, cross-attention, feed-forward networks), language modeling head (vocabulary projection to 32,128 tokens), and shared token embeddings are trainable. No layers are frozen. Full fine-tuning is chosen because the text-to-SQL task requires adapting all representations to understand both natural language semantics and SQL syntax, and the small model size allows efficient full fine-tuning. |
| **Hyperparameters** | Learning rate: 1e-3 (0.001), chosen after experimentation showed 1e-1 caused instability. Batch size: 16. Optimizer: AdamW with betas=(0.9, 0.999), eps=1e-8, weight_decay=0. Learning rate scheduler: Cosine annealing with 0 warmup epochs, decaying from 1e-3 to near zero. Maximum epochs: 20. Early stopping: Patience of 3-5 epochs based on Record F1 on development set, with best model checkpoint saved. Loss function: CrossEntropyLoss with ignore_index=-100 to ignore padded positions. Generation: Greedy decoding (num_beams=1), max_length=512, early_stopping=True. Best model achieved 83.6% Record F1 at epoch 16. |

---

## Copy-paste ready format (for LaTeX):

```
Data processing & Natural language queries are prepended with the prefix "translate English to SQL: " before tokenization to provide task specification. Both encoder inputs and decoder targets are truncated to a maximum length of 512 tokens. End-of-sequence (EOS) tokens are explicitly appended to decoder targets if not present. During batch collation, sequences are dynamically padded: encoder/decoder inputs use pad\_token\_id (0), while decoder targets are padded with -100 to ignore padded positions in loss computation. Decoder inputs are created by shifting targets by one position and prepending pad\_token\_id as the initial token (teacher-forcing). No additional preprocessing (lowercasing, formatting) is applied. \\

Tokenization & T5TokenizerFast from HuggingFace Transformers (google-t5/t5-small) is used for both encoder and decoder tokenization. This SentencePiece-based tokenizer handles both natural language and SQL syntax effectively. Encoder inputs are tokenized after prefix addition with truncation to max\_length=512. Decoder targets (SQL queries) are tokenized directly without prefix, also with max\_length=512. Special tokens include pad\_token\_id (0) for padding and EOS token (1) for sequence termination. The default T5 tokenizer is used without modifications as it naturally handles SQL syntax through its learned subword vocabulary. \\

Architecture & T5ForConditionalGeneration (small variant, 60M parameters) from google-t5/t5-small is fine-tuned. The model consists of 6 encoder layers, 6 decoder layers, 8 attention heads, and hidden dimension 512. Full model fine-tuning is performed: all parameters in encoder (6 transformer layers with self-attention, feed-forward networks, layer normalization), decoder (6 transformer layers with self-attention, cross-attention, feed-forward networks), language modeling head (vocabulary projection to 32,128 tokens), and shared token embeddings are trainable. No layers are frozen. Full fine-tuning is chosen because the text-to-SQL task requires adapting all representations to understand both natural language semantics and SQL syntax, and the small model size allows efficient full fine-tuning. \\

Hyperparameters & Learning rate: 1e-3 (0.001), chosen after experimentation showed 1e-1 caused instability. Batch size: 16. Optimizer: AdamW with betas=(0.9, 0.999), eps=1e-8, weight\_decay=0. Learning rate scheduler: Cosine annealing with 0 warmup epochs, decaying from 1e-3 to near zero. Maximum epochs: 20. Early stopping: Patience of 3-5 epochs based on Record F1 on development set, with best model checkpoint saved. Loss function: CrossEntropyLoss with ignore\_index=-100 to ignore padded positions. Generation: Greedy decoding (num\_beams=1), max\_length=512, early\_stopping=True. Best model achieved 83.6\% Record F1 at epoch 16. \\
```

